
import io, os, math
import numpy as np
import pandas as pd
import streamlit as st

st.set_page_config(page_title="AHPâ€“ç†µæƒ ç»¼åˆè¯„ä»·ç³»ç»Ÿï¼ˆå¤šå±‚çº§å¼•å¯¼ç‰ˆï¼‰", layout="wide")
st.title("AHPâ€“ç†µæƒ ç»¼åˆè¯„ä»·ç³»ç»Ÿï¼ˆå¤šå±‚çº§å¼•å¯¼ç‰ˆï¼‰")

# ---------------- Utilities ----------------
RI_TABLE = {1:0.00,2:0.00,3:0.58,4:0.90,5:1.12,6:1.24,7:1.32,8:1.41,9:1.45,10:1.49,11:1.51,12:1.48}

def eig_weight_consistency(A):
    vals, vecs = np.linalg.eig(A)
    idx = np.argmax(np.real(vals))
    lam = np.real(vals[idx])
    w = np.real(vecs[:, idx])
    w = np.maximum(w, 0)
    w = w / (w.sum() if w.sum()>0 else 1.0)
    n = A.shape[0]
    CI = (lam - n) / (n - 1) if n>1 else 0.0
    RI = RI_TABLE.get(n, 1.49)
    CR = CI/RI if RI>0 else 0.0
    return w, lam, CI, CR

def row_arith_weight(A):
    col_sum = A.sum(axis=0); col_sum[col_sum==0] = 1.0
    N = A/col_sum
    w = N.mean(axis=1)
    w = np.maximum(w, 0); w = w/(w.sum() if w.sum()>0 else 1.0)
    return w

def row_geo_weight(A):
    with np.errstate(divide='ignore', invalid='ignore'):
        g = np.prod(A, axis=1)**(1.0/A.shape[1])
    g = np.nan_to_num(g, nan=0.0, posinf=0.0, neginf=0.0)
    w = g/(g.sum() if g.sum()>0 else 1.0)
    return w

def ruc_fuse(wE, wA, wG, CR, prior=(0.4,0.3,0.3), metric="L1", mode="geo"):
    W = np.vstack([wE,wA,wG]); wbar = W.mean(axis=0)
    if metric=="L2":
        d = np.array([1.0+np.linalg.norm(W[0]-wbar,2),
                      1.0+np.linalg.norm(W[1]-wbar,2),
                      1.0+np.linalg.norm(W[2]-wbar,2)])
    else:
        d = np.array([1.0+np.linalg.norm(W[0]-wbar,1),
                      1.0+np.linalg.norm(W[1]-wbar,1),
                      1.0+np.linalg.norm(W[2]-wbar,1)])
    pE,pA,pG = prior
    rE = max(0.0, min(1.0, 1.0-CR/0.10))
    gtilde = (np.array([pE,pA,pG])*np.array([rE,1.0,1.0]))/d
    gamma = gtilde/(gtilde.sum() if gtilde.sum()>0 else 1.0)
    if mode=="lin":
        wf = gamma[0]*wE + gamma[1]*wA + gamma[2]*wG
        wf = np.maximum(wf,0); wf = wf/(wf.sum() if wf.sum()>0 else 1.0)
        return wf, gamma
    else:
        wf = (np.maximum(wE,1e-12)**gamma[0])*(np.maximum(wA,1e-12)**gamma[1])*(np.maximum(wG,1e-12)**gamma[2])
        wf = wf/(wf.sum() if wf.sum()>0 else 1.0)
        return wf, gamma

def normalize_minmax(x):
    xmin, xmax = np.nanmin(x), np.nanmax(x)
    rng = xmax-xmin
    if rng<=0: return np.zeros_like(x)
    return (x-xmin)/(rng+1e-12)

# -------------- Step 0: Load hierarchy --------------
st.sidebar.header("æ­¥éª¤å¯¼èˆª")
st.sidebar.markdown("""
## ğŸ§­ æ­¥éª¤å¯¼èˆª

**â… . æ•°æ®å‡†å¤‡**
- ğŸ—‚ï¸ åŠ è½½å±‚çº§å…³ç³»ï¼ˆhierarchy.csvï¼‰

**â…¡. æƒé‡è®¡ç®—**
- âš–ï¸ AHP ä¸¤ä¸¤æ¯”è¾ƒï¼ˆé€å±‚å¼•å¯¼ï¼‰
- ğŸ“ˆ ç†µæƒæ³•ï¼ˆæ ¹æ®ä¼ä¸šå¾—åˆ†è‡ªåŠ¨è®¡ç®—ï¼‰

**â…¢. æƒé‡èåˆä¸ç»“æœ**
- ğŸ”„ ä¸»å®¢è§‚æƒé‡èåˆï¼ˆAHP Ã— ç†µæƒï¼‰
- ğŸ ç»¼åˆè¯„åˆ†ä¸ç»“æœå¯¼å‡º
""")

uploaded_hier = st.file_uploader("ä¸Šä¼ å±‚çº§å…³ç³» CSVï¼ˆåˆ—ï¼šlevel,id,name,parent_id,typeï¼›å·²ä¸ºä½ ç”Ÿæˆ hierarchy.csvï¼‰", type=["csv"])
default_path = "hierarchy.csv"
if uploaded_hier is not None:
    df_hier = pd.read_csv(uploaded_hier)
elif os.path.exists(default_path):
    df_hier = pd.read_csv(default_path)
    st.caption("å·²åŠ è½½æœ¬åœ° hierarchy.csv")
else:
    st.error("è¯·å…ˆä¸Šä¼ å±‚çº§å…³ç³»CSVã€‚"); st.stop()

# Validate
need_cols = {"level","id","name","parent_id","type"}
if not need_cols.issubset(set([c.lower() for c in df_hier.columns])):
    st.error("CSV å¿…é¡»åŒ…å«åˆ—ï¼šlevel,id,name,parent_id,type"); st.stop()

# Standardize cols
df_hier.columns = [c.lower() for c in df_hier.columns]
df_hier["level"] = df_hier["level"].astype(int)

L1_nodes = df_hier[df_hier["level"]==1].copy()
L2_nodes = df_hier[df_hier["level"]==2].copy()
L3_nodes = df_hier[df_hier["level"]==3].copy()

st.success(f"å·²åŠ è½½å±‚çº§ï¼šä¸€çº§ {len(L1_nodes)}ï¼ŒäºŒçº§ {len(L2_nodes)}ï¼Œä¸‰çº§ {len(L3_nodes)}ï¼ˆå¶å­æŒ‡æ ‡ï¼‰ã€‚")

# Groupings
children_L1 = {pid: L2_nodes[L2_nodes["parent_id"]==pid]["id"].tolist() for pid in L1_nodes["id"]}
children_L2 = {pid: L3_nodes[L3_nodes["parent_id"]==pid]["id"].tolist() for pid in L2_nodes["id"]}
name_map = df_hier.set_index("id")["name"].to_dict()
type_map = df_hier[df_hier["level"]==3].set_index("id")["type"].to_dict()

# -------------- Step 1: AHP pairwise per parent --------------
st.header("æ­¥éª¤ä¸€ï¼šAHP ä¸¤ä¸¤æ¯”è¾ƒï¼ˆé€çˆ¶èŠ‚ç‚¹ï¼‰")
priorE = st.slider("RUC-AHP å…ˆéªŒï¼šç‰¹å¾å€¼æ³•", 0.0, 1.0, 0.4, 0.05)
priorA = st.slider("RUC-AHP å…ˆéªŒï¼šç®—æœ¯å¹³å‡æ³•", 0.0, 1.0, 0.3, 0.05)
priorG = st.slider("RUC-AHP å…ˆéªŒï¼šå‡ ä½•å¹³å‡æ³•", 0.0, 1.0, 0.3, 0.05)
metric = st.selectbox("åç¦»åº¦åº¦é‡", ["L1","L2"], index=0)
mode = st.selectbox("èåˆæ–¹å¼", ["geoï¼ˆä¹˜æ€§ï¼Œæ¨èï¼‰","linï¼ˆçº¿æ€§ï¼‰"], index=0)
mode_key = "geo" if mode.startswith("geo") else "lin"

def render_pairwise(node_ids, title_key):
    n = len(node_ids)
    st.markdown(f"**{title_key}** â€” æˆå‘˜ï¼š{[name_map[x] for x in node_ids]}")
    keyprefix = "m_"+title_key
    # initialize A in session
    if keyprefix not in st.session_state or st.session_state[keyprefix]["shape"]!=(n,n) or st.session_state[keyprefix]["ids"]!=tuple(node_ids):
        A = np.ones((n,n), dtype=float)
        st.session_state[keyprefix] = {"A":A, "shape":(n,n), "ids":tuple(node_ids)}
    else:
        A = st.session_state[keyprefix]["A"]

    with st.form("form_"+keyprefix):
        cols = st.columns(n+1)
        cols[0].markdown("**i/j**")
        for j in range(n): cols[j+1].markdown(f"**{name_map[node_ids[j]]}**")
        for i in range(n):
            row = st.columns(n+1)
            row[0].write(f"**{name_map[node_ids[i]]}**")
            for j in range(n):
                if i==j:
                    row[j+1].write("1")
                    A[i,j]=1.0
                elif i<j:
                    k = f"{keyprefix}_{i}_{j}"
                    default = float(A[i,j]) if A[i,j]!=1.0 else 1.0
                    v = row[j+1].number_input(f"{name_map[node_ids[i]]} vs {name_map[node_ids[j]]}",
                                              min_value=1.0, max_value=9.0, value=float(default), step=1.0, key=k)
                    A[i,j]=float(v); A[j,i]=1.0/float(v)
                else:
                    row[j+1].write(f"{A[i,j]:.3f}")
        st.form_submit_button("æ›´æ–°çŸ©é˜µ")
    st.session_state[keyprefix]["A"]=A

    # methods & fuse
    wE, lam, CI, CR = eig_weight_consistency(A)
    wA = row_arith_weight(A)
    wG = row_geo_weight(A)
    wf, gamma = ruc_fuse(wE,wA,wG,CR, prior=(priorE,priorA,priorG), metric=metric, mode=mode_key)

    df = pd.DataFrame({
        "id": node_ids,
        "name": [name_map[x] for x in node_ids],
        "E": wE, "A": wA, "G": wG, "fused": wf
    }).set_index("id")
    m1,m2,m3,m4 = st.columns(4)
    m1.metric("Î»_max", f"{lam:.4f}"); m2.metric("CI", f"{CI:.4f}")
    m3.metric("CR(â‰¤0.10ä¸ºä½³)", f"{CR:.4f}"); m4.metric("n", f"{n}")
    st.dataframe(df[["name","E","A","G","fused"]], use_container_width=True)
    return df["fused"]

# L1 matrix
st.subheader("â‘  ä¸€çº§ï¼šESG / KPI / DPI é‡è¦æ€§æ¯”è¾ƒ")
w_L1 = render_pairwise(L1_nodes["id"].tolist(), "Level1")

# L2 per parent
st.subheader("â‘¡ äºŒçº§ï¼šåœ¨å„è‡ªä¸€çº§ä¹‹ä¸‹çš„æ¯”è¾ƒ")
w_L2_local = {}
for pid, children in children_L1.items():
    with st.expander(f"çˆ¶èŠ‚ç‚¹ï¼š{name_map[pid]}ï¼ˆåŒ…å« {len(children)} ä¸ªäºŒçº§ï¼‰", expanded=False):
        w = render_pairwise(children, f"Level2_{pid}")
        w_L2_local.update(w.to_dict())

# L3 per parent
st.subheader("â‘¢ ä¸‰çº§ï¼šåœ¨å„è‡ªäºŒçº§ä¹‹ä¸‹çš„æ¯”è¾ƒ")
w_L3_local = {}
for pid, children in children_L2.items():
    with st.expander(f"çˆ¶èŠ‚ç‚¹ï¼ˆäºŒçº§ï¼‰ï¼š{name_map[pid]}ï¼ˆåŒ…å« {len(children)} ä¸ªä¸‰çº§ï¼‰", expanded=False):
        w = render_pairwise(children, f"Level3_{pid}")
        w_L3_local.update(w.to_dict())

# Compute global AHP weights for leaves
st.subheader("â‘£ è®¡ç®—ä¸‰çº§æŒ‡æ ‡å…¨å±€ AHP æƒé‡")
# First make series for L1 & L2
wL1 = pd.Series(w_L1.to_dict(), name="wL1")  # id->weight
wL2 = pd.Series(w_L2_local, name="wL2_local")
wL3 = pd.Series(w_L3_local, name="wL3_local")

# Build global mapping
global_ahp = {}
for leaf_id in L3_nodes["id"]:
    parent2 = L3_nodes[L3_nodes["id"]==leaf_id]["parent_id"].iloc[0]
    parent1 = L2_nodes[L2_nodes["id"]==parent2]["parent_id"].iloc[0]
    global_w = wL1[parent1]*wL2[parent2]*wL3[leaf_id]
    global_ahp[leaf_id] = global_w
global_ahp = pd.Series(global_ahp).sort_index()
global_ahp = global_ahp / (global_ahp.sum() if global_ahp.sum()>0 else 1.0)

df_ahp_global = pd.DataFrame({
    "id": global_ahp.index,
    "name": [name_map[x] for x in global_ahp.index],
    "type": [type_map.get(x,"benefit") for x in global_ahp.index],
    "AHP_global": global_ahp.values
}).set_index("id")
st.dataframe(df_ahp_global, use_container_width=True)

# ---------------- Step 2: Upload company scores ----------------
st.header("æ­¥éª¤äºŒï¼šä¸Šä¼ ä¼ä¸šÃ—ä¸‰çº§æŒ‡æ ‡å¾—åˆ†")
st.caption("è¦æ±‚ï¼šåˆ—åä¸ºå¶å­æŒ‡æ ‡ idï¼ˆå¦‚ T1..T37ï¼‰æˆ–ä¸­æ–‡åç§°ï¼ˆç³»ç»Ÿä¼šè‡ªåŠ¨è¯†åˆ«ï¼‰ï¼Œè¡Œ=å…¬å¸")
scores_file = st.file_uploader("ä¸Šä¼  CSVï¼ˆCompanyåˆ— + 37åˆ—æŒ‡æ ‡ï¼‰", type=["csv"], key="scorescsv")
winsor = st.slider("Winsorize å»æå€¼ï¼ˆæ¯ç«¯ç™¾åˆ†æ¯”ï¼‰", 0.0, 20.0, 0.0, 1.0)

if scores_file is None:
    st.info("ç­‰å¾…ä¸Šä¼ å…¬å¸å¾—åˆ†è¡¨â€¦â€¦")
    st.stop()

df_scores = pd.read_csv(scores_file)
# Detect company col
first_col = df_scores.columns[0]
if first_col.lower() in ["company","firm","ä¼ä¸š","å…¬å¸","name"]:
    df_scores = df_scores.set_index(first_col)
# Try map columns by id or name
cols_by_id = [c for c in df_scores.columns if c in L3_nodes["id"].values]
if len(cols_by_id)==len(L3_nodes):
    S = df_scores[cols_by_id].copy()
else:
    # try by name -> id
    name_to_id = {name_map[i]:i for i in L3_nodes["id"]}
    try_cols = []
    for c in df_scores.columns:
        if c in name_to_id:
            try_cols.append(name_to_id[c])
    if len(try_cols)==len(L3_nodes):
        S = df_scores[ [name_map[i] for i in L3_nodes["id"]] ].copy()
        S.columns = L3_nodes["id"].tolist()
    else:
        st.error("åˆ—åéœ€ä¸º T1..T37 æˆ–å¯¹åº”ä¸­æ–‡åç§°ï¼Œä¸”å¿…é¡»å®Œæ•´åŒ¹é…ã€‚"); st.stop()

# Winsorize
if winsor>0:
    low, high = winsor, 100-winsor
    for col in S.columns:
        lo = np.nanpercentile(S[col].values, low)
        hi = np.nanpercentile(S[col].values, high)
        S[col] = S[col].clip(lo, hi)

st.write("åŸå§‹æ‰“åˆ†ï¼ˆSï¼‰")
st.dataframe(S, use_container_width=True)

# ---------------- Step 3: Entropy weights ----------------
st.header("æ­¥éª¤ä¸‰ï¼šç†µæƒï¼ˆå®¢è§‚æƒé‡ï¼‰")
# Normalize per indicator with direction
Z = pd.DataFrame(index=S.index, columns=S.columns, dtype=float)
for col in S.columns:
    x = S[col].astype(float).values
    z = normalize_minmax(x)
    if type_map.get(col,"benefit")=="cost":
        z = 1.0 - z
    Z[col] = np.maximum(z, 0.0)

st.write("æ ‡å‡†åŒ–çŸ©é˜µ Zï¼ˆç”¨äºç†µæƒä¸åŠ æƒè¯„åˆ†ï¼‰")
st.dataframe(Z, use_container_width=True)

m = Z.shape[0]; k = 1.0/np.log(m) if m>1 else 0.0
P = Z.div(Z.sum(axis=0).replace(0.0, np.nan), axis=1).fillna(0.0)
E = -k * (P.replace(0.0, np.nan).applymap(lambda v: v*np.log(v) if v>0 else 0.0)).sum(axis=0).fillna(0.0)
D = 1.0 - E
ENT = (D / (D.sum() if D.sum()>0 else 1.0)).rename("ENT")
st.dataframe(ENT.to_frame(), use_container_width=True)

# ---------------- Step 4: Fusion ----------------
st.header("æ­¥éª¤å››ï¼šä¸»å®¢è§‚æƒé‡èåˆ")
alpha = st.slider("AHP å æ¯” Î±", 0.0, 1.0, 0.5, 0.05)
ahp_vec = df_ahp_global["AHP_global"].reindex(Z.columns).values
ent_vec = ENT.reindex(Z.columns).values
w_fused = (np.maximum(ahp_vec,1e-12)**alpha) * (np.maximum(ent_vec,1e-12)**(1-alpha))
w_fused = w_fused / (w_fused.sum() if w_fused.sum()>0 else 1.0)

weights_tbl = pd.DataFrame({
    "id": Z.columns,
    "name": [name_map[i] for i in Z.columns],
    "type": [type_map.get(i,"benefit") for i in Z.columns],
    "AHP_global": ahp_vec,
    "ENT": ENT.reindex(Z.columns).values,
    "FUSED": w_fused
}).set_index("id")
st.dataframe(weights_tbl, use_container_width=True)

# ---------------- Step 5: Final scores ----------------
st.header("æ­¥éª¤äº”ï¼šæœ€ç»ˆå…¬å¸ç»¼åˆå¾—åˆ†ä¸å¯¼å‡º")
final_scores = (Z * w_fused).sum(axis=1).to_frame(name="FinalScore").sort_values("FinalScore", ascending=False)
st.dataframe(final_scores, use_container_width=True)

# Downloads
buf_weights = io.BytesIO(weights_tbl.to_csv(index=True).encode("utf-8"))
buf_scores = io.BytesIO(final_scores.to_csv(index=True).encode("utf-8"))
st.download_button("ä¸‹è½½ æƒé‡è¡¨ (CSV)", data=buf_weights, file_name="weights_fused.csv", mime="text/csv")
st.download_button("ä¸‹è½½ å…¬å¸è¯„åˆ† (CSV)", data=buf_scores, file_name="company_scores.csv", mime="text/csv")

with st.expander("æ–¹æ³•è¯´æ˜ / Notes"):
    st.markdown("""
- **å¤šå±‚çº§ AHP**ï¼šæŒ‰çˆ¶èŠ‚ç‚¹é€å—ä¸¤ä¸¤æ¯”è¾ƒï¼Œè®¡ç®—æœ¬åœ°æƒé‡ä¸ CRï¼›é€šè¿‡ RUC-AHP ç¨³å¥èåˆå¾—åˆ°æ¯å—çš„æœ¬åœ°æƒé‡ï¼›å±‚å±‚ç›¸ä¹˜å¾—åˆ°**ä¸‰çº§å…¨å±€ AHP æƒé‡**ã€‚
- **ç†µæƒæ³•**ï¼šåŸºäºä¼ä¸šÃ—æŒ‡æ ‡å¾—åˆ†çš„æ ‡å‡†åŒ–çŸ©é˜µ Z è®¡ç®—å®¢è§‚æƒé‡ ENTã€‚æˆæœ¬å‹æŒ‡æ ‡è‡ªåŠ¨åšæ–¹å‘åè½¬ã€‚
- **ä¸»å®¢è§‚èåˆ**ï¼š`w âˆ (AHP^Î±)*(ENT^(1-Î±))`ï¼Œé»˜è®¤ Î±=0.5ï¼ˆç›¸ä¹˜å¼€å¹³æ–¹ï¼‰ã€‚
- **æœ€ç»ˆå¾—åˆ†**ï¼š`Score(company) = Î£ w_i * Z_{company,i}`ã€‚
""")
